package resource

import (
	"context"
	"fmt"
	"net"
	"sync"
	"sync/atomic"
	"time"

	"go.uber.org/zap"
)

// NetworkResourceManager ç½‘ç»œèµ„æºç®¡ç†å™¨
type NetworkResourceManager struct {
	config *NetworkResourceConfig
	logger *zap.Logger

	// è¿æ¥æ± 
	pools      map[string]*ConnectionPool
	poolsMutex sync.RWMutex

	// è¿æ¥ç›‘æ§
	activeConns map[string]*ManagedConnection
	connsMutex  sync.RWMutex

	// ç»Ÿè®¡ä¿¡æ¯
	stats struct {
		totalConnections  atomic.Int64
		activeConnections atomic.Int64
		pooledConnections atomic.Int64
		connectionReuse   atomic.Int64
		connectionTimeout atomic.Int64
		connectionErrors  atomic.Int64
	}

	// æ§åˆ¶
	ctx    context.Context
	cancel context.CancelFunc
	wg     sync.WaitGroup
}

// NetworkResourceConfig ç½‘ç»œèµ„æºé…ç½®
type NetworkResourceConfig struct {
	// è¿æ¥æ± é…ç½®
	MaxPoolSize          int `json:"max_pool_size"`
	MinPoolSize          int `json:"min_pool_size"`
	MaxIdleConnections   int `json:"max_idle_connections"`
	MaxActiveConnections int `json:"max_active_connections"`

	// è¶…æ—¶é…ç½®
	ConnectionTimeout time.Duration `json:"connection_timeout"`
	IdleTimeout       time.Duration `json:"idle_timeout"`
	MaxLifetime       time.Duration `json:"max_lifetime"`
	KeepAliveTimeout  time.Duration `json:"keep_alive_timeout"`

	// å¥åº·æ£€æŸ¥
	HealthCheckInterval time.Duration `json:"health_check_interval"`
	HealthCheckTimeout  time.Duration `json:"health_check_timeout"`
	MaxRetries          int           `json:"max_retries"`

	// ç›‘æ§é…ç½®
	EnableMetrics   bool          `json:"enable_metrics"`
	MetricsInterval time.Duration `json:"metrics_interval"`
}

// DefaultNetworkResourceConfig é»˜è®¤ç½‘ç»œèµ„æºé…ç½®
func DefaultNetworkResourceConfig() *NetworkResourceConfig {
	return &NetworkResourceConfig{
		MaxPoolSize:          100,
		MinPoolSize:          5,
		MaxIdleConnections:   20,
		MaxActiveConnections: 80,
		ConnectionTimeout:    30 * time.Second,
		IdleTimeout:          5 * time.Minute,
		MaxLifetime:          1 * time.Hour,
		KeepAliveTimeout:     30 * time.Second,
		HealthCheckInterval:  1 * time.Minute,
		HealthCheckTimeout:   5 * time.Second,
		MaxRetries:           3,
		EnableMetrics:        true,
		MetricsInterval:      30 * time.Second,
	}
}

// ConnectionPool è¿æ¥æ± 
type ConnectionPool struct {
	target  string
	network string
	config  *NetworkResourceConfig
	logger  *zap.Logger

	// è¿æ¥å­˜å‚¨
	idle   []*ManagedConnection
	active map[string]*ManagedConnection
	mutex  sync.RWMutex

	// ç»Ÿè®¡
	stats struct {
		created  atomic.Int64
		closed   atomic.Int64
		reused   atomic.Int64
		timeouts atomic.Int64
		errors   atomic.Int64
	}

	// æ§åˆ¶
	ctx    context.Context
	cancel context.CancelFunc
	wg     sync.WaitGroup
}

// ManagedConnection æ‰˜ç®¡è¿æ¥
type ManagedConnection struct {
	id        string
	conn      net.Conn
	target    string
	network   string
	createdAt time.Time
	lastUsed  atomic.Int64
	useCount  atomic.Int64
	healthy   atomic.Bool
	inUse     atomic.Bool
	pool      *ConnectionPool

	// å¥åº·æ£€æŸ¥
	lastHealthCheck atomic.Int64
	healthErrors    atomic.Int32
}

// NewNetworkResourceManager åˆ›å»ºç½‘ç»œèµ„æºç®¡ç†å™¨
func NewNetworkResourceManager(config *NetworkResourceConfig) *NetworkResourceManager {
	if config == nil {
		config = DefaultNetworkResourceConfig()
	}

	return &NetworkResourceManager{
		config:      config,
		logger:      zap.L().Named("network-resource-manager"),
		pools:       make(map[string]*ConnectionPool),
		activeConns: make(map[string]*ManagedConnection),
	}
}

// Start å¯åŠ¨ç½‘ç»œèµ„æºç®¡ç†å™¨
func (nrm *NetworkResourceManager) Start(ctx context.Context) error {
	nrm.ctx, nrm.cancel = context.WithCancel(ctx)

	// å¯åŠ¨ç›‘æ§åç¨‹
	if nrm.config.EnableMetrics {
		nrm.wg.Add(1)
		go nrm.metricsLoop()
	}

	// å¯åŠ¨å¥åº·æ£€æŸ¥åç¨‹
	nrm.wg.Add(1)
	go nrm.healthCheckLoop()

	// å¯åŠ¨æ¸…ç†åç¨‹
	nrm.wg.Add(1)
	go nrm.cleanupLoop()

	nrm.logger.Info("ğŸŒ ç½‘ç»œèµ„æºç®¡ç†å™¨å¯åŠ¨",
		zap.Int("max_pool_size", nrm.config.MaxPoolSize),
		zap.Int("max_active_connections", nrm.config.MaxActiveConnections),
		zap.Duration("connection_timeout", nrm.config.ConnectionTimeout))

	return nil
}

// Stop åœæ­¢ç½‘ç»œèµ„æºç®¡ç†å™¨
func (nrm *NetworkResourceManager) Stop() error {
	nrm.logger.Info("ğŸ›‘ æ­£åœ¨åœæ­¢ç½‘ç»œèµ„æºç®¡ç†å™¨...")

	if nrm.cancel != nil {
		nrm.cancel()
	}

	// å…³é—­æ‰€æœ‰è¿æ¥æ± 
	nrm.poolsMutex.Lock()
	for _, pool := range nrm.pools {
		pool.Close()
	}
	nrm.poolsMutex.Unlock()

	// ç­‰å¾…åç¨‹ç»“æŸ
	nrm.wg.Wait()

	nrm.logger.Info("âœ… ç½‘ç»œèµ„æºç®¡ç†å™¨å·²åœæ­¢")
	return nil
}

// GetConnection è·å–è¿æ¥
func (nrm *NetworkResourceManager) GetConnection(network, target string) (*ManagedConnection, error) {
	poolKey := fmt.Sprintf("%s://%s", network, target)

	// è·å–æˆ–åˆ›å»ºè¿æ¥æ± 
	pool := nrm.getOrCreatePool(poolKey, network, target)

	// ä»æ± ä¸­è·å–è¿æ¥
	conn, err := pool.Get()
	if err != nil {
		nrm.stats.connectionErrors.Add(1)
		return nil, err
	}

	// æ³¨å†Œæ´»è·ƒè¿æ¥
	nrm.registerActiveConnection(conn)

	nrm.stats.activeConnections.Add(1)
	return conn, nil
}

// ReturnConnection å½’è¿˜è¿æ¥
func (nrm *NetworkResourceManager) ReturnConnection(conn *ManagedConnection) error {
	if conn == nil {
		return fmt.Errorf("è¿æ¥ä¸èƒ½ä¸ºç©º")
	}

	// ä»æ´»è·ƒè¿æ¥ä¸­ç§»é™¤
	nrm.unregisterActiveConnection(conn)

	// å½’è¿˜åˆ°æ± ä¸­
	err := conn.pool.Put(conn)
	if err != nil {
		return err
	}

	nrm.stats.activeConnections.Add(-1)
	return nil
}

// getOrCreatePool è·å–æˆ–åˆ›å»ºè¿æ¥æ± 
func (nrm *NetworkResourceManager) getOrCreatePool(poolKey, network, target string) *ConnectionPool {
	nrm.poolsMutex.RLock()
	if pool, exists := nrm.pools[poolKey]; exists {
		nrm.poolsMutex.RUnlock()
		return pool
	}
	nrm.poolsMutex.RUnlock()

	nrm.poolsMutex.Lock()
	defer nrm.poolsMutex.Unlock()

	// åŒé‡æ£€æŸ¥
	if pool, exists := nrm.pools[poolKey]; exists {
		return pool
	}

	// åˆ›å»ºæ–°çš„è¿æ¥æ± 
	pool := NewConnectionPool(network, target, nrm.config)
	pool.Start(nrm.ctx)
	nrm.pools[poolKey] = pool

	nrm.logger.Debug("åˆ›å»ºè¿æ¥æ± ",
		zap.String("pool_key", poolKey),
		zap.String("network", network),
		zap.String("target", target))

	return pool
}

// registerActiveConnection æ³¨å†Œæ´»è·ƒè¿æ¥
func (nrm *NetworkResourceManager) registerActiveConnection(conn *ManagedConnection) {
	nrm.connsMutex.Lock()
	nrm.activeConns[conn.id] = conn
	nrm.connsMutex.Unlock()
}

// unregisterActiveConnection æ³¨é”€æ´»è·ƒè¿æ¥
func (nrm *NetworkResourceManager) unregisterActiveConnection(conn *ManagedConnection) {
	nrm.connsMutex.Lock()
	delete(nrm.activeConns, conn.id)
	nrm.connsMutex.Unlock()
}

// metricsLoop æŒ‡æ ‡å¾ªç¯
func (nrm *NetworkResourceManager) metricsLoop() {
	defer nrm.wg.Done()

	ticker := time.NewTicker(nrm.config.MetricsInterval)
	defer ticker.Stop()

	for {
		select {
		case <-nrm.ctx.Done():
			return
		case <-ticker.C:
			nrm.logMetrics()
		}
	}
}

// healthCheckLoop å¥åº·æ£€æŸ¥å¾ªç¯
func (nrm *NetworkResourceManager) healthCheckLoop() {
	defer nrm.wg.Done()

	ticker := time.NewTicker(nrm.config.HealthCheckInterval)
	defer ticker.Stop()

	for {
		select {
		case <-nrm.ctx.Done():
			return
		case <-ticker.C:
			nrm.performHealthChecks()
		}
	}
}

// cleanupLoop æ¸…ç†å¾ªç¯
func (nrm *NetworkResourceManager) cleanupLoop() {
	defer nrm.wg.Done()

	ticker := time.NewTicker(1 * time.Minute)
	defer ticker.Stop()

	for {
		select {
		case <-nrm.ctx.Done():
			return
		case <-ticker.C:
			nrm.performCleanup()
		}
	}
}

// performHealthChecks æ‰§è¡Œå¥åº·æ£€æŸ¥
func (nrm *NetworkResourceManager) performHealthChecks() {
	nrm.connsMutex.RLock()
	connections := make([]*ManagedConnection, 0, len(nrm.activeConns))
	for _, conn := range nrm.activeConns {
		connections = append(connections, conn)
	}
	nrm.connsMutex.RUnlock()

	for _, conn := range connections {
		if conn.shouldHealthCheck() {
			go conn.performHealthCheck()
		}
	}
}

// performCleanup æ‰§è¡Œæ¸…ç†
func (nrm *NetworkResourceManager) performCleanup() {
	nrm.poolsMutex.RLock()
	pools := make([]*ConnectionPool, 0, len(nrm.pools))
	for _, pool := range nrm.pools {
		pools = append(pools, pool)
	}
	nrm.poolsMutex.RUnlock()

	for _, pool := range pools {
		pool.cleanup()
	}
}

// logMetrics è®°å½•æŒ‡æ ‡
func (nrm *NetworkResourceManager) logMetrics() {
	stats := nrm.GetStats()

	nrm.logger.Info("ğŸ“Š ç½‘ç»œèµ„æºç»Ÿè®¡",
		zap.Int64("total_connections", stats["total_connections"].(int64)),
		zap.Int64("active_connections", stats["active_connections"].(int64)),
		zap.Int64("pooled_connections", stats["pooled_connections"].(int64)),
		zap.Int64("connection_reuse", stats["connection_reuse"].(int64)),
		zap.Int("active_pools", len(nrm.pools)))
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (nrm *NetworkResourceManager) GetStats() map[string]interface{} {
	nrm.poolsMutex.RLock()
	poolStats := make(map[string]interface{})
	totalPooled := int64(0)
	for key, pool := range nrm.pools {
		stats := pool.GetStats()
		poolStats[key] = stats
		totalPooled += stats["idle_count"].(int64)
	}
	nrm.poolsMutex.RUnlock()

	return map[string]interface{}{
		"total_connections":  nrm.stats.totalConnections.Load(),
		"active_connections": nrm.stats.activeConnections.Load(),
		"pooled_connections": totalPooled,
		"connection_reuse":   nrm.stats.connectionReuse.Load(),
		"connection_timeout": nrm.stats.connectionTimeout.Load(),
		"connection_errors":  nrm.stats.connectionErrors.Load(),
		"active_pools":       len(nrm.pools),
		"pool_stats":         poolStats,
		"config":             nrm.config,
	}
}

// NewConnectionPool åˆ›å»ºè¿æ¥æ± 
func NewConnectionPool(network, target string, config *NetworkResourceConfig) *ConnectionPool {
	return &ConnectionPool{
		target:  target,
		network: network,
		config:  config,
		logger:  zap.L().Named("connection-pool").With(zap.String("target", target)),
		idle:    make([]*ManagedConnection, 0, config.MaxIdleConnections),
		active:  make(map[string]*ManagedConnection),
	}
}

// Start å¯åŠ¨è¿æ¥æ± 
func (cp *ConnectionPool) Start(ctx context.Context) {
	cp.ctx, cp.cancel = context.WithCancel(ctx)

	// é¢„çƒ­è¿æ¥æ± 
	cp.wg.Add(1)
	go cp.warmup()
}

// Get è·å–è¿æ¥
func (cp *ConnectionPool) Get() (*ManagedConnection, error) {
	cp.mutex.Lock()
	defer cp.mutex.Unlock()

	// å°è¯•ä»ç©ºé—²è¿æ¥ä¸­è·å–
	for len(cp.idle) > 0 {
		conn := cp.idle[len(cp.idle)-1]
		cp.idle = cp.idle[:len(cp.idle)-1]

		// æ£€æŸ¥è¿æ¥æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
		if conn.isValid() {
			conn.inUse.Store(true)
			conn.lastUsed.Store(time.Now().Unix())
			conn.useCount.Add(1)
			cp.active[conn.id] = conn
			cp.stats.reused.Add(1)
			return conn, nil
		}

		// è¿æ¥æ— æ•ˆï¼Œå…³é—­å®ƒ
		conn.close()
	}

	// æ£€æŸ¥æ´»è·ƒè¿æ¥æ•°é™åˆ¶
	if len(cp.active) >= cp.config.MaxActiveConnections {
		return nil, fmt.Errorf("è¿æ¥æ± å·²æ»¡ï¼Œæ´»è·ƒè¿æ¥æ•°: %d", len(cp.active))
	}

	// åˆ›å»ºæ–°è¿æ¥
	conn, err := cp.createConnection()
	if err != nil {
		cp.stats.errors.Add(1)
		return nil, err
	}

	conn.inUse.Store(true)
	cp.active[conn.id] = conn
	cp.stats.created.Add(1)
	return conn, nil
}

// Put å½’è¿˜è¿æ¥
func (cp *ConnectionPool) Put(conn *ManagedConnection) error {
	if conn == nil {
		return fmt.Errorf("è¿æ¥ä¸èƒ½ä¸ºç©º")
	}

	cp.mutex.Lock()
	defer cp.mutex.Unlock()

	// ä»æ´»è·ƒè¿æ¥ä¸­ç§»é™¤
	delete(cp.active, conn.id)
	conn.inUse.Store(false)

	// æ£€æŸ¥è¿æ¥æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
	if !conn.isValid() || len(cp.idle) >= cp.config.MaxIdleConnections {
		conn.close()
		return nil
	}

	// åŠ å…¥ç©ºé—²è¿æ¥
	cp.idle = append(cp.idle, conn)
	return nil
}

// createConnection åˆ›å»ºè¿æ¥
func (cp *ConnectionPool) createConnection() (*ManagedConnection, error) {
	// åˆ›å»ºå¸¦è¶…æ—¶çš„è¿æ¥
	dialer := &net.Dialer{
		Timeout: cp.config.ConnectionTimeout,
	}

	conn, err := dialer.DialContext(cp.ctx, cp.network, cp.target)
	if err != nil {
		return nil, fmt.Errorf("åˆ›å»ºè¿æ¥å¤±è´¥: %w", err)
	}

	// åˆ›å»ºæ‰˜ç®¡è¿æ¥
	managedConn := &ManagedConnection{
		id:        fmt.Sprintf("conn-%s-%d", cp.target, time.Now().UnixNano()),
		conn:      conn,
		target:    cp.target,
		network:   cp.network,
		createdAt: time.Now(),
		pool:      cp,
	}

	managedConn.lastUsed.Store(time.Now().Unix())
	managedConn.healthy.Store(true)
	managedConn.lastHealthCheck.Store(time.Now().Unix())

	return managedConn, nil
}

// warmup é¢„çƒ­è¿æ¥æ± 
func (cp *ConnectionPool) warmup() {
	defer cp.wg.Done()

	for i := 0; i < cp.config.MinPoolSize; i++ {
		conn, err := cp.createConnection()
		if err != nil {
			cp.logger.Warn("é¢„çƒ­è¿æ¥å¤±è´¥", zap.Error(err))
			continue
		}

		cp.mutex.Lock()
		cp.idle = append(cp.idle, conn)
		cp.mutex.Unlock()
	}

	cp.logger.Debug("è¿æ¥æ± é¢„çƒ­å®Œæˆ",
		zap.Int("min_size", cp.config.MinPoolSize),
		zap.Int("created", len(cp.idle)))
}

// cleanup æ¸…ç†è¿æ¥æ± 
func (cp *ConnectionPool) cleanup() {
	cp.mutex.Lock()
	defer cp.mutex.Unlock()

	now := time.Now()

	// æ¸…ç†è¿‡æœŸçš„ç©ºé—²è¿æ¥
	validIdle := cp.idle[:0]
	for _, conn := range cp.idle {
		if conn.isExpired(now) {
			conn.close()
		} else {
			validIdle = append(validIdle, conn)
		}
	}
	cp.idle = validIdle

	// æ£€æŸ¥æ´»è·ƒè¿æ¥
	for id, conn := range cp.active {
		if conn.isExpired(now) && !conn.inUse.Load() {
			delete(cp.active, id)
			conn.close()
		}
	}
}

// Close å…³é—­è¿æ¥æ± 
func (cp *ConnectionPool) Close() {
	if cp.cancel != nil {
		cp.cancel()
	}

	cp.mutex.Lock()
	defer cp.mutex.Unlock()

	// å…³é—­æ‰€æœ‰ç©ºé—²è¿æ¥
	for _, conn := range cp.idle {
		conn.close()
	}
	cp.idle = nil

	// å…³é—­æ‰€æœ‰æ´»è·ƒè¿æ¥
	for _, conn := range cp.active {
		conn.close()
	}
	cp.active = make(map[string]*ManagedConnection)

	cp.wg.Wait()
}

// GetStats è·å–è¿æ¥æ± ç»Ÿè®¡
func (cp *ConnectionPool) GetStats() map[string]interface{} {
	cp.mutex.RLock()
	idleCount := len(cp.idle)
	activeCount := len(cp.active)
	cp.mutex.RUnlock()

	return map[string]interface{}{
		"target":       cp.target,
		"network":      cp.network,
		"idle_count":   int64(idleCount),
		"active_count": int64(activeCount),
		"created":      cp.stats.created.Load(),
		"closed":       cp.stats.closed.Load(),
		"reused":       cp.stats.reused.Load(),
		"timeouts":     cp.stats.timeouts.Load(),
		"errors":       cp.stats.errors.Load(),
	}
}

// ManagedConnection æ–¹æ³•

// isValid æ£€æŸ¥è¿æ¥æ˜¯å¦æœ‰æ•ˆ
func (mc *ManagedConnection) isValid() bool {
	if !mc.healthy.Load() {
		return false
	}

	// æ£€æŸ¥è¿æ¥æ˜¯å¦å…³é—­
	if mc.conn == nil {
		return false
	}

	// æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§ç”Ÿå‘½å‘¨æœŸ
	if time.Since(mc.createdAt) > mc.pool.config.MaxLifetime {
		return false
	}

	return true
}

// isExpired æ£€æŸ¥è¿æ¥æ˜¯å¦è¿‡æœŸ
func (mc *ManagedConnection) isExpired(now time.Time) bool {
	// æ£€æŸ¥ç©ºé—²è¶…æ—¶
	lastUsed := time.Unix(mc.lastUsed.Load(), 0)
	if now.Sub(lastUsed) > mc.pool.config.IdleTimeout {
		return true
	}

	// æ£€æŸ¥æœ€å¤§ç”Ÿå‘½å‘¨æœŸ
	if now.Sub(mc.createdAt) > mc.pool.config.MaxLifetime {
		return true
	}

	return false
}

// shouldHealthCheck æ˜¯å¦åº”è¯¥è¿›è¡Œå¥åº·æ£€æŸ¥
func (mc *ManagedConnection) shouldHealthCheck() bool {
	lastCheck := time.Unix(mc.lastHealthCheck.Load(), 0)
	return time.Since(lastCheck) > mc.pool.config.HealthCheckInterval
}

// performHealthCheck æ‰§è¡Œå¥åº·æ£€æŸ¥
func (mc *ManagedConnection) performHealthCheck() {
	mc.lastHealthCheck.Store(time.Now().Unix())

	// ç®€å•çš„å¥åº·æ£€æŸ¥ï¼šå°è¯•è®¾ç½®è¯»å–è¶…æ—¶
	if mc.conn != nil {
		if err := mc.conn.SetReadDeadline(time.Now().Add(mc.pool.config.HealthCheckTimeout)); err != nil {
			mc.healthErrors.Add(1)
			if mc.healthErrors.Load() > 3 {
				mc.healthy.Store(false)
			}
		} else {
			mc.healthErrors.Store(0)
			mc.healthy.Store(true)
		}
	}
}

// close å…³é—­è¿æ¥
func (mc *ManagedConnection) close() {
	if mc.conn != nil {
		mc.conn.Close()
		mc.conn = nil
	}
	mc.healthy.Store(false)
}

// GetConnection è·å–åº•å±‚è¿æ¥
func (mc *ManagedConnection) GetConnection() net.Conn {
	mc.lastUsed.Store(time.Now().Unix())
	return mc.conn
}

// GetID è·å–è¿æ¥ID
func (mc *ManagedConnection) GetID() string {
	return mc.id
}

// GetStats è·å–è¿æ¥ç»Ÿè®¡
func (mc *ManagedConnection) GetStats() map[string]interface{} {
	return map[string]interface{}{
		"id":            mc.id,
		"target":        mc.target,
		"network":       mc.network,
		"created_at":    mc.createdAt,
		"last_used":     time.Unix(mc.lastUsed.Load(), 0),
		"use_count":     mc.useCount.Load(),
		"healthy":       mc.healthy.Load(),
		"in_use":        mc.inUse.Load(),
		"health_errors": mc.healthErrors.Load(),
	}
}
