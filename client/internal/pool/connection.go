package pool

import (
	"context"
	"fmt"
	"net"
	"sync"
	"time"

	"gkipass/client/internal/transport"

	"go.uber.org/zap"
)

// MonitoredConnection è¢«ç›‘æ§çš„è¿æ¥åŒ…è£…å™¨
type MonitoredConnection struct {
	net.Conn
	pooledConn *PooledConnection
	pool       *AdaptivePool
}

// NewMonitoredConnection åˆ›å»ºè¢«ç›‘æ§çš„è¿æ¥
func NewMonitoredConnection(pooledConn *PooledConnection, pool *AdaptivePool) *MonitoredConnection {
	return &MonitoredConnection{
		Conn:       pooledConn.conn,
		pooledConn: pooledConn,
		pool:       pool,
	}
}

// Read è¯»å–æ•°æ®å¹¶è®°å½•ç»Ÿè®¡
func (m *MonitoredConnection) Read(b []byte) (int, error) {
	n, err := m.Conn.Read(b)

	// è®°å½•ç»Ÿè®¡
	if n > 0 {
		m.pooledConn.bytesIn.Add(int64(n))
		m.pooledConn.packetsRecv.Add(1)
	}

	if err != nil {
		m.pooledConn.errorCount.Add(1)
		// å¯èƒ½çš„ä¸¢åŒ…æ£€æµ‹
		if isNetworkError(err) {
			m.pooledConn.packetsLost.Add(1)
		}
	}

	return n, err
}

// Write å†™å…¥æ•°æ®å¹¶è®°å½•ç»Ÿè®¡
func (m *MonitoredConnection) Write(b []byte) (int, error) {
	start := time.Now()
	n, err := m.Conn.Write(b)
	rtt := time.Since(start) * 2 // ç®€åŒ–çš„RTTä¼°ç®—

	// è®°å½•ç»Ÿè®¡
	if n > 0 {
		m.pooledConn.bytesOut.Add(int64(n))
		m.pooledConn.packetsSent.Add(1)
	}

	if err != nil {
		m.pooledConn.errorCount.Add(1)
		if isNetworkError(err) {
			m.pooledConn.packetsLost.Add(1)
		}
	} else {
		// è®°å½•RTTæ ·æœ¬
		m.recordRTT(rtt)
	}

	m.pooledConn.requestCount.Add(1)
	return n, err
}

// Close å…³é—­è¿æ¥æ—¶é‡Šæ”¾å›æ± ä¸­
func (m *MonitoredConnection) Close() error {
	// ä¸çœŸæ­£å…³é—­è¿æ¥ï¼Œè€Œæ˜¯é‡Šæ”¾å›æ± ä¸­
	m.pool.ReleaseConnection(m.pooledConn)
	return nil
}

// ForceClose å¼ºåˆ¶å…³é—­è¿æ¥
func (m *MonitoredConnection) ForceClose() error {
	return m.Conn.Close()
}

// recordRTT è®°å½•RTTæ ·æœ¬
func (m *MonitoredConnection) recordRTT(rtt time.Duration) {
	m.pooledConn.rttMutex.Lock()
	defer m.pooledConn.rttMutex.Unlock()

	m.pooledConn.rttSamples = append(m.pooledConn.rttSamples, rtt)
	if len(m.pooledConn.rttSamples) > 20 {
		// ä¿æŒæœ€è¿‘20ä¸ªæ ·æœ¬
		copy(m.pooledConn.rttSamples, m.pooledConn.rttSamples[1:])
		m.pooledConn.rttSamples = m.pooledConn.rttSamples[:19]
	}
}

// GetConnectionInfo è·å–è¿æ¥ä¿¡æ¯
func (m *MonitoredConnection) GetConnectionInfo() map[string]interface{} {
	quality := m.pool.getConnectionQuality(m.pooledConn)

	return map[string]interface{}{
		"id":           m.pooledConn.id,
		"created_at":   m.pooledConn.createdAt,
		"last_used":    m.pooledConn.lastUsed,
		"bytes_in":     m.pooledConn.bytesIn.Load(),
		"bytes_out":    m.pooledConn.bytesOut.Load(),
		"requests":     m.pooledConn.requestCount.Load(),
		"errors":       m.pooledConn.errorCount.Load(),
		"packets_sent": m.pooledConn.packetsSent.Load(),
		"packets_lost": m.pooledConn.packetsLost.Load(),
		"packets_recv": m.pooledConn.packetsRecv.Load(),
		"quality":      quality,
	}
}

// isNetworkError åˆ¤æ–­æ˜¯å¦ä¸ºç½‘ç»œé”™è¯¯
func isNetworkError(err error) bool {
	if err == nil {
		return false
	}

	// ç®€åŒ–çš„ç½‘ç»œé”™è¯¯æ£€æµ‹
	netErr, ok := err.(net.Error)
	return ok && (netErr.Timeout() || netErr.Temporary())
}

// PoolManager è¿æ¥æ± ç®¡ç†å™¨
type PoolManager struct {
	pools  map[string]*AdaptivePool
	mutex  sync.RWMutex
	logger *zap.Logger
	config *PoolConfig

	// åŠ¨æ€é…ç½®
	configWatcher chan *PoolConfig
	ctx           context.Context
	cancel        context.CancelFunc
	wg            sync.WaitGroup
}

// Manager ç®¡ç†å™¨ç±»å‹åˆ«å
type Manager = PoolManager

// New åˆ›å»ºè¿æ¥æ± ç®¡ç†å™¨ (é€šç”¨æ¥å£)
func New(config *PoolConfig) *Manager {
	return NewPoolManager()
}

// NewPoolManager åˆ›å»ºè¿æ¥æ± ç®¡ç†å™¨
func NewPoolManager() *PoolManager {
	return &PoolManager{
		pools:         make(map[string]*AdaptivePool),
		logger:        zap.L().Named("pool-manager"),
		config:        DefaultPoolConfig(),
		configWatcher: make(chan *PoolConfig, 1),
	}
}

// NewPoolManagerWithConfig ä½¿ç”¨æŒ‡å®šé…ç½®åˆ›å»ºè¿æ¥æ± ç®¡ç†å™¨
func NewPoolManagerWithConfig(config *PoolConfig) *PoolManager {
	if err := config.Validate(); err != nil {
		zap.L().Error("è¿æ¥æ± é…ç½®éªŒè¯å¤±è´¥", zap.Error(err))
		config = DefaultPoolConfig()
	}

	return &PoolManager{
		pools:         make(map[string]*AdaptivePool),
		logger:        zap.L().Named("pool-manager"),
		config:        config,
		configWatcher: make(chan *PoolConfig, 1),
	}
}

// GetOrCreatePool è·å–æˆ–åˆ›å»ºè¿æ¥æ± 
func (pm *PoolManager) GetOrCreatePool(targetAddr string, transportType transport.TransportType, transportMgr *transport.Manager) *AdaptivePool {
	poolKey := fmt.Sprintf("%s-%s", string(transportType), targetAddr)

	pm.mutex.RLock()
	if pool, exists := pm.pools[poolKey]; exists {
		pm.mutex.RUnlock()
		return pool
	}
	pm.mutex.RUnlock()

	// åˆ›å»ºæ–°çš„è¿æ¥æ± 
	pm.mutex.Lock()
	defer pm.mutex.Unlock()

	// åŒé‡æ£€æŸ¥
	if pool, exists := pm.pools[poolKey]; exists {
		return pool
	}

	pool := NewAdaptivePool(targetAddr, transportType, transportMgr)

	// åº”ç”¨é…ç½®
	if pm.config != nil {
		pm.config.Apply(pool)
		pool.loadBalanceStrategy = ParseLoadBalanceStrategy(pm.config.LoadBalanceStrategy)
		pool.preWarmingEnabled = pm.config.EnablePreWarming
		pool.preWarmingTargets = pm.config.PreWarmingTargets
	}

	pm.pools[poolKey] = pool

	pm.logger.Info("åˆ›å»ºæ–°è¿æ¥æ± ",
		zap.String("key", poolKey),
		zap.String("target", targetAddr),
		zap.String("transport", string(transportType)),
		zap.String("strategy", pool.loadBalanceStrategy.String()))

	return pool
}

// GetConnection ä»æ± ä¸­è·å–è¿æ¥
func (pm *PoolManager) GetConnection(targetAddr string, transportType transport.TransportType, transportMgr *transport.Manager) (*MonitoredConnection, error) {
	pool := pm.GetOrCreatePool(targetAddr, transportType, transportMgr)

	pooledConn, err := pool.GetConnection()
	if err != nil {
		return nil, err
	}

	return NewMonitoredConnection(pooledConn, pool), nil
}

// Start å¯åŠ¨è¿æ¥æ± ç®¡ç†å™¨
func (pm *PoolManager) Start() error {
	pm.ctx, pm.cancel = context.WithCancel(context.Background())

	// å¯åŠ¨é…ç½®ç›‘æ§
	pm.wg.Add(1)
	go pm.configWatchLoop()

	pm.logger.Info("âœ… è¿æ¥æ± ç®¡ç†å™¨å·²å¯åŠ¨")
	return nil
}

// Stop åœæ­¢è¿æ¥æ± ç®¡ç†å™¨
func (pm *PoolManager) Stop() error {
	pm.logger.Info("ğŸ›‘ æ­£åœ¨åœæ­¢è¿æ¥æ± ç®¡ç†å™¨")

	if pm.cancel != nil {
		pm.cancel()
	}

	// åœæ­¢æ‰€æœ‰è¿æ¥æ± 
	if err := pm.StopAll(); err != nil {
		pm.logger.Error("åœæ­¢è¿æ¥æ± å¤±è´¥", zap.Error(err))
	}

	// ç­‰å¾…åå°ä»»åŠ¡ç»“æŸ
	pm.wg.Wait()

	pm.logger.Info("âœ… è¿æ¥æ± ç®¡ç†å™¨å·²åœæ­¢")
	return nil
}

// StartAll å¯åŠ¨æ‰€æœ‰è¿æ¥æ± 
func (pm *PoolManager) StartAll(ctx context.Context) error {
	pm.mutex.RLock()
	defer pm.mutex.RUnlock()

	for _, pool := range pm.pools {
		if err := pool.Start(ctx); err != nil {
			pm.logger.Error("å¯åŠ¨è¿æ¥æ± å¤±è´¥", zap.Error(err))
			return err
		}
	}

	pm.logger.Info("æ‰€æœ‰è¿æ¥æ± å¯åŠ¨å®Œæˆ", zap.Int("count", len(pm.pools)))
	return nil
}

// StopAll åœæ­¢æ‰€æœ‰è¿æ¥æ± 
func (pm *PoolManager) StopAll() error {
	pm.mutex.Lock()
	defer pm.mutex.Unlock()

	for key, pool := range pm.pools {
		if err := pool.Stop(); err != nil {
			pm.logger.Error("åœæ­¢è¿æ¥æ± å¤±è´¥", zap.String("key", key), zap.Error(err))
		}
	}

	// æ¸…ç©ºæ± æ˜ å°„
	pm.pools = make(map[string]*AdaptivePool)
	pm.logger.Info("æ‰€æœ‰è¿æ¥æ± å·²åœæ­¢")
	return nil
}

// GetStats è·å–æ‰€æœ‰è¿æ¥æ± ç»Ÿè®¡
func (pm *PoolManager) GetStats() map[string]interface{} {
	pm.mutex.RLock()
	defer pm.mutex.RUnlock()

	stats := map[string]interface{}{
		"pool_count": len(pm.pools),
		"pools":      make(map[string]interface{}),
	}

	for key, pool := range pm.pools {
		stats["pools"].(map[string]interface{})[key] = pool.GetStats()
	}

	return stats
}

// configWatchLoop é…ç½®ç›‘æ§å¾ªç¯
func (pm *PoolManager) configWatchLoop() {
	defer pm.wg.Done()

	for {
		select {
		case newConfig := <-pm.configWatcher:
			pm.applyNewConfig(newConfig)
		case <-pm.ctx.Done():
			return
		}
	}
}

// UpdateConfig æ›´æ–°é…ç½®
func (pm *PoolManager) UpdateConfig(config *PoolConfig) error {
	if err := config.Validate(); err != nil {
		return fmt.Errorf("é…ç½®éªŒè¯å¤±è´¥: %w", err)
	}

	select {
	case pm.configWatcher <- config:
		pm.logger.Info("ğŸ”„ æäº¤é…ç½®æ›´æ–°è¯·æ±‚")
		return nil
	default:
		return fmt.Errorf("é…ç½®æ›´æ–°æ¸ é“å¿™ç¢")
	}
}

// applyNewConfig åº”ç”¨æ–°é…ç½®
func (pm *PoolManager) applyNewConfig(newConfig *PoolConfig) {
	pm.logger.Info("ğŸ”§ åº”ç”¨æ–°é…ç½®")

	pm.mutex.Lock()
	oldConfig := pm.config
	pm.config = newConfig
	pm.mutex.Unlock()

	// å¯¹æ‰€æœ‰ç°æœ‰è¿æ¥æ± åº”ç”¨æ–°é…ç½®
	pm.mutex.RLock()
	for key, pool := range pm.pools {
		pm.applyConfigToPool(pool, oldConfig, newConfig)
		pm.logger.Debug("æ± é…ç½®å·²åº”ç”¨", zap.String("pool", key))
	}
	pm.mutex.RUnlock()

	pm.logger.Info("âœ… é…ç½®æ›´æ–°å®Œæˆ")
}

// applyConfigToPool å¯¹å•ä¸ªè¿æ¥æ± åº”ç”¨é…ç½®
func (pm *PoolManager) applyConfigToPool(pool *AdaptivePool, oldConfig, newConfig *PoolConfig) {
	// åº”ç”¨åŸºæœ¬é…ç½®
	newConfig.Apply(pool)

	// æ›´æ–°è´Ÿè½½å‡è¡¡ç­–ç•¥
	if oldConfig.LoadBalanceStrategy != newConfig.LoadBalanceStrategy {
		pool.SetLoadBalanceStrategy(ParseLoadBalanceStrategy(newConfig.LoadBalanceStrategy))
	}

	// æ›´æ–°é¢„çƒ­é…ç½®
	pool.preWarmingEnabled = newConfig.EnablePreWarming
	pool.preWarmingTargets = newConfig.PreWarmingTargets

	// å¦‚æœå¯ç”¨äº†é¢„çƒ­ä¸”ä¹‹å‰æ²¡å¯ç”¨ï¼Œæ‰§è¡Œé¢„çƒ­
	if newConfig.EnablePreWarming && !oldConfig.EnablePreWarming {
		go pool.performPreWarming()
	}
}

// GetConfig è·å–å½“å‰é…ç½®
func (pm *PoolManager) GetConfig() *PoolConfig {
	pm.mutex.RLock()
	defer pm.mutex.RUnlock()
	return pm.config
}
